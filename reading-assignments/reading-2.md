Technical tools are considered “objective” because they are designed to output the optimal solution according to the given data and algorithm. In other words, what you feed, what you get. In this sense, some obvious prejudices and discriminations to human can be somehow avoided by adjusting the variety of input dataset, the features, or the chosen algorithm. Yet what about those discriminations even hidden to human’s eyes at one time? Chances are that we humans don’t even recognize all the contributing factors of the problem, let alone the machines could be programmed to learn from the data and eliminate all the bias. Machines are initialized under human guidance so far, therefore bias is destined to exist in terms of such mechanism. According to Virginia Eubanks, once machines and technical tools created the “triage model”, they are actually stating a priority. Instead of fairing out discriminations in social/welfare systems, they are essentially allocating resources according to this priority. Discriminations, therefore, would never be eliminated unless the priority is absolutely objective and fair, which is almost impossible to set under the complex social contexts and individual differences.
In this sense, from the terms “the state doesn’t need a cop to kill a person” and “electronic incarceration”, I can only see the expectations on avoiding cops’ or prison guards’ mistakes. But who can kill a person and guard the incarceration then? Is it more a question about “who should” or about “why would”?

One similar example that I can think of is [“social credit” monitoring](https://www.youtube.com/watch?v=GsIdUGWsXn8&feature=emb_title). Zhima Credit (also known as Sesame Credit), is also a program run by Ant Financial (Alibaba). It measures customer trustworthiness according to their online behavior and credit history, and offers benefits including low-interest loans, preferential access to bike and car-sharing services, etc to those with high credit scores. In some cities where local government instituted mandatory pilot social credit systems, “redlists” are set to encourage trustworthy behavior among individuals, businesses, and even government departments. Accordingly, blacklists exist for those committing minor infractions and behave untrustworthily, and punishment includes but is not limited to public sector employment limitations and government benefits refusal. Though the overall system seems to strike a balance between both citizens’ and organizations’ sides and ensures their credits for the sake of societal processes, whether the data are shared with authorities, whether “blacklists” deprives individual rights and liberty and other debates still requires emphasis.
